{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "* Build a static relationship list containing the most common relationship names\n",
    "* Sentence segmentation\n",
    "* Tokenization\n",
    "* Search for names in the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python\\lib\\site-packages\\nltk\\tag\\stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use nltk.tag.corenlp.CoreNLPPOSTagger or nltk.tag.corenlp.CoreNLPNERTagger instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pprint\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "nlp = spacy.load('de')\n",
    "#nlp = spacy.load('de_core_news_sm')\n",
    "#form fuzzywuzzy import fuzz\n",
    "\n",
    "model = '../models/dewac_175m_600.crf.ser.gz'\n",
    "#model = '../models/hgc_175m_600.crf.ser.gz'\n",
    "#model = '../models/german.conll.germeval2014.hgc_175m_600.crf.ser.gz'\n",
    "\n",
    "st = StanfordNERTagger(model,\n",
    "                       '../models/stanford-ner.jar',\n",
    "                       encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_long = u'''Meine Enkelin Lisa und mein Enkel Lukas fliegen morgen nach London. Sie sind zum ersten Mal in England.\n",
    "Peter und Maria gehen morgen ins Kino. Ich und mein Sohn gehen heute zum Fußball. Ich bin geboren zu York im Jahre 1632, als Kind angesehener Leute, die ursprünglich nicht aus jener Gegend stammten. \n",
    "Mein Vater, ein Ausländer, aus Bremen gebürtig, hatte sich zuerst in Hull niedergelassen, war dort als Kaufmann zu \n",
    "hübschem Vermögen gekommen und dann, nachdem er sein Geschäft aufgegeben hatte, nach York gezogen. \n",
    "Hier heiratete er meine Mutter, eine geborene Robinson.\n",
    "Ich hatte zwei ältere Brüder. Der eine von ihnen, welcher als Oberstleutnant bei einem englischen, \n",
    "früher von dem berühmten Oberst Lockhart befehligten Infanterieregiment in Flandern diente, \n",
    "fiel in der Schlacht bei Dünkirchen. Was aus dem jüngeren geworden ist, habe ich ebenso wenig in Erfahrung bringen können, \n",
    "als meine Eltern je Kenntnisse von meinen eignen Schicksalen erhalten haben.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u'''Meine Enkelin Lisa und mein Enkel Lukas fliegen morgen nach London. Sie sind zum ersten Mal in England. \n",
    "Peter und Maria gehen morgen ins Kino. Ich und mein Sohn gehen heute zum Fußball.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_list = ['vater', 'mutter', 'sohn', 'tochter', 'bruder', 'schwester', 'enkel', 'enkelin', 'nichte', \n",
    "                     'neffe', 'onkel', 'tante']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "me_list = ['ich', 'meine', 'mein']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meine DET nk\n",
      "Enkelin PROPN pnc\n",
      "Lisa PROPN sb\n",
      "und CONJ cd\n",
      "mein DET nk\n",
      "Enkel NOUN cj\n",
      "Lukas PROPN ag\n",
      "fliegen VERB ROOT\n",
      "morgen ADV mo\n",
      "nach ADP mo\n",
      "London PROPN nk\n",
      ". PUNCT punct\n",
      "Sie PRON sb\n",
      "sind AUX ROOT\n",
      "zum ADP mo\n",
      "ersten ADJ nk\n",
      "Mal NOUN nk\n",
      "in ADP mo\n",
      "England PROPN nk\n",
      ". PUNCT punct\n",
      "\n",
      " SPACE \n",
      "Peter PROPN sb\n",
      "und CONJ cd\n",
      "Maria PROPN cj\n",
      "gehen VERB ROOT\n",
      "morgen ADV mo\n",
      "ins ADP mo\n",
      "Kino NOUN nk\n",
      ". PUNCT punct\n",
      "Ich PRON sb\n",
      "und CONJ cd\n",
      "mein DET nk\n",
      "Sohn NOUN cj\n",
      "gehen VERB ROOT\n",
      "heute ADV mo\n",
      "zum ADP mo\n",
      "Fußball NOUN nk\n",
      ". PUNCT punct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Meine', 'DET'),\n",
       " ('Enkelin', 'PROPN'),\n",
       " ('Lisa', 'PROPN'),\n",
       " ('und', 'CONJ'),\n",
       " ('mein', 'DET'),\n",
       " ('Enkel', 'NOUN'),\n",
       " ('Lukas', 'PROPN'),\n",
       " ('fliegen', 'VERB'),\n",
       " ('morgen', 'ADV'),\n",
       " ('nach', 'ADP'),\n",
       " ('London', 'PROPN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Sie', 'PRON'),\n",
       " ('sind', 'AUX'),\n",
       " ('zum', 'ADP'),\n",
       " ('ersten', 'ADJ'),\n",
       " ('Mal', 'NOUN'),\n",
       " ('in', 'ADP'),\n",
       " ('England', 'PROPN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('\\n', 'SPACE'),\n",
       " ('Peter', 'PROPN'),\n",
       " ('und', 'CONJ'),\n",
       " ('Maria', 'PROPN'),\n",
       " ('gehen', 'VERB'),\n",
       " ('morgen', 'ADV'),\n",
       " ('ins', 'ADP'),\n",
       " ('Kino', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Ich', 'PRON'),\n",
       " ('und', 'CONJ'),\n",
       " ('mein', 'DET'),\n",
       " ('Sohn', 'NOUN'),\n",
       " ('gehen', 'VERB'),\n",
       " ('heute', 'ADV'),\n",
       " ('zum', 'ADP'),\n",
       " ('Fußball', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "pos_tagged_sentences = []\n",
    "for token in doc:\n",
    "    pos_tuple = (token.text, token.pos_)\n",
    "    pos_tagged_sentences.append(pos_tuple)\n",
    "    print(token.text, token.pos_, token.dep_,)\n",
    "    \n",
    "pos_tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meine nk Lisa PROPN []\n",
      "Enkelin pnc Lisa PROPN []\n",
      "Lisa sb fliegen VERB [Meine, Enkelin, und]\n",
      "und cd Lisa PROPN [Enkel]\n",
      "mein nk Enkel NOUN []\n",
      "Enkel cj und CONJ [mein, Lukas]\n",
      "Lukas ag Enkel NOUN []\n",
      "fliegen ROOT fliegen VERB [Lisa, morgen, nach, .]\n",
      "morgen mo fliegen VERB []\n",
      "nach mo fliegen VERB [London]\n",
      "London nk nach ADP []\n",
      ". punct fliegen VERB []\n",
      "Sie sb sind AUX []\n",
      "sind ROOT sind AUX [Sie, zum, in, .]\n",
      "zum mo sind AUX [Mal]\n",
      "ersten nk Mal NOUN []\n",
      "Mal nk zum ADP [ersten]\n",
      "in mo sind AUX [England]\n",
      "England nk in ADP []\n",
      ". punct sind AUX [\n",
      "]\n",
      "\n",
      "  . PUNCT []\n",
      "Peter sb gehen VERB [und]\n",
      "und cd Peter PROPN [Maria]\n",
      "Maria cj und CONJ []\n",
      "gehen ROOT gehen VERB [Peter, morgen, ins, .]\n",
      "morgen mo gehen VERB []\n",
      "ins mo gehen VERB [Kino]\n",
      "Kino nk ins ADP []\n",
      ". punct gehen VERB []\n",
      "Ich sb gehen VERB [und]\n",
      "und cd Ich PRON [Sohn]\n",
      "mein nk Sohn NOUN []\n",
      "Sohn cj und CONJ [mein]\n",
      "gehen ROOT gehen VERB [Ich, heute, zum, .]\n",
      "heute mo gehen VERB []\n",
      "zum mo gehen VERB [Fußball]\n",
      "Fußball nk zum ADP []\n",
      ". punct gehen VERB []\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all sentences containing two or more named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using spaCy NER  DEPRECATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Meine Enkelin Lisa', 'MISC'), ('Lukas', 'PER'), ('London', 'LOC')]\n",
      "[('England', 'LOC')]\n",
      "[('Ich bin geboren zu York im Jahre 1632', 'MISC')]\n",
      "[('Mein', 'PER'), ('Bremen', 'LOC'), ('Hull', 'LOC'), ('\\nhübschem Vermögen', 'PER'), ('York', 'LOC')]\n",
      "[('Robinson', 'PER')]\n",
      "[('Brüder', 'MISC')]\n",
      "[('englischen', 'MISC'), ('\\n', 'MISC'), ('Lockhart', 'PER'), ('Flandern', 'LOC'), ('\\n', 'MISC'), ('Schlacht bei Dünkirchen', 'MISC')]\n",
      "[('\\n', 'MISC')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_to_check = []\n",
    "labeled_sentences = []\n",
    "for sentence in sent_tokenize(text):\n",
    "    labeled_sentence = []\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        tuple = (ent.text, ent.label_)\n",
    "        labeled_sentence.append((ent.text, ent.label_))\n",
    "    \n",
    "    print(labeled_sentence)\n",
    "    for label in labeled_sentences:\n",
    "        if 'PER' in label[1]:\n",
    "            sentences_to_check.append(labeled_sentence)\n",
    "\n",
    "sentences_to_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Stanford NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Meine', 'PME'), ('Enkelin', 'REL'), ('Lisa', 'PER'), ('Enkel', 'REL'), ('Lukas', 'PER')], [('Peter', 'PER'), ('Maria', 'PER')], [('ich', 'PME'), ('meine', 'PME')]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ich', 'PME'), ('meine', 'PME')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_to_check = []\n",
    "\n",
    "\n",
    "for sentence in sent_tokenize(text):\n",
    "    ner_tuples = st.tag(sentence.split())\n",
    "    rel_tuples = []\n",
    "    label = 0\n",
    "    for ner_tuple in ner_tuples:        \n",
    "        if 'I-PER' in ner_tuple:\n",
    "            label += 1\n",
    "            tuple = (ner_tuple[0], 'PER')\n",
    "            rel_tuples.append(tuple)\n",
    "        elif ner_tuple[0].lower() in relationship_list:\n",
    "            label += 1\n",
    "            tuple = (ner_tuple[0], 'REL')\n",
    "            rel_tuples.append(tuple)\n",
    "        elif ner_tuple[0].lower() in me_list:\n",
    "            label += 1\n",
    "            tuple = (ner_tuple[0], 'PME')\n",
    "            rel_tuples.append(tuple)\n",
    "    \n",
    "    if label >= 2:\n",
    "        sentences_to_check.append(rel_tuples)\n",
    "\n",
    "sentences_to_check\n",
    "\n",
    "#for sentence in sentences_to_check:\n",
    "#    tokenized_text = word_tokenize(utterance)\n",
    "#    classified = st.tag(tokenized_text)\n",
    "    \n",
    "#classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Meine', 'O'),\n",
       "  ('Enkelin', 'O'),\n",
       "  ('Lisa', 'I-PER'),\n",
       "  ('und', 'O'),\n",
       "  ('mein', 'O'),\n",
       "  ('Enkel', 'O'),\n",
       "  ('Lukas', 'I-PER'),\n",
       "  ('fliegen', 'O'),\n",
       "  ('morgen', 'O'),\n",
       "  ('nach', 'O'),\n",
       "  ('London.', 'I-LOC')],\n",
       " [('Der', 'O'),\n",
       "  ('eine', 'O'),\n",
       "  ('von', 'O'),\n",
       "  ('ihnen,', 'O'),\n",
       "  ('welcher', 'O'),\n",
       "  ('als', 'O'),\n",
       "  ('Oberstleutnant', 'O'),\n",
       "  ('bei', 'O'),\n",
       "  ('einem', 'O'),\n",
       "  ('englischen,', 'O'),\n",
       "  ('früher', 'O'),\n",
       "  ('von', 'O'),\n",
       "  ('dem', 'O'),\n",
       "  ('berühmten', 'O'),\n",
       "  ('Oberst', 'O'),\n",
       "  ('Lockhart', 'I-PER'),\n",
       "  ('befehligten', 'O'),\n",
       "  ('Infanterieregiment', 'O'),\n",
       "  ('in', 'O'),\n",
       "  ('Flandern', 'I-LOC'),\n",
       "  ('diente,', 'O'),\n",
       "  ('fiel', 'O'),\n",
       "  ('in', 'O'),\n",
       "  ('der', 'O'),\n",
       "  ('Schlacht', 'O'),\n",
       "  ('bei', 'O'),\n",
       "  ('Dünkirchen.', 'I-LOC')]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in sentences_to_check:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meine Enkelin Lisa und mein Enkel Lukas fliegen morgen nach London.\n",
      "Meine Enkelin Lisa MISC\n",
      "Lukas PER\n",
      "London LOC\n",
      "Meine Enkelin Lisa und mein Enkel Lukas fliegen morgen nach London.\n",
      "Meine Enkelin Lisa MISC\n",
      "Lukas PER\n",
      "London LOC\n",
      "Mein Vater, ein Ausländer, aus Bremen gebürtig, hatte sich zuerst in Hull niedergelassen, war dort als Kaufmann zu \n",
      "hübschem Vermögen gekommen und dann, nachdem er sein Geschäft aufgegeben hatte, nach York gezogen.\n",
      "Mein PER\n",
      "Bremen LOC\n",
      "Hull LOC\n",
      "\n",
      "hübschem Vermögen PER\n",
      "York LOC\n",
      "Hier heiratete er meine Mutter, eine geborene Robinson.\n",
      "Robinson PER\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences_to_check:\n",
    "    doc = nlp(sentence)\n",
    "    print(sentence)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<list_iterator at 0x1eb9ac32160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "path_to_jar = '../models/stanford-parser.jar'\n",
    "path_to_models_jar = '../models/stanford-german-corenlp-models.jar'\n",
    "\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "result = dependency_parser.raw_parse_sents('Ich habe eine Enkelin namens Lisa.')\n",
    "dep = result.__next__()\n",
    "\n",
    "result\n",
    "#list(dep.triples())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meine nk Lisa PROPN []\n",
      "Enkelin pnc Lisa PROPN []\n",
      "Lisa sb fliegen VERB [Meine, Enkelin, und]\n",
      "und cd Lisa PROPN [Enkel]\n",
      "mein nk Enkel NOUN []\n",
      "Enkel cj und CONJ [mein, Lukas]\n",
      "Lukas ag Enkel NOUN []\n",
      "fliegen ROOT fliegen VERB [Lisa, morgen, nach, .]\n",
      "morgen mo fliegen VERB []\n",
      "nach mo fliegen VERB [London]\n",
      "London nk nach ADP []\n",
      ". punct fliegen VERB []\n",
      "Sie sb sind AUX []\n",
      "sind ROOT sind AUX [Sie, zum, in, .]\n",
      "zum mo sind AUX [Mal]\n",
      "ersten nk Mal NOUN []\n",
      "Mal nk zum ADP [ersten]\n",
      "in mo sind AUX [England]\n",
      "England nk in ADP []\n",
      ". punct sind AUX []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meine    <--nk--- Lisa\n",
      "Enkelin  <--pnc-- Lisa\n",
      "Lisa     <--sb--- fliegen\n",
      "und      <--cd--- Lisa\n",
      "mein     <--nk--- Enkel\n",
      "Enkel    <--cj--- und\n",
      "Lukas    <--ag--- Enkel\n",
      "fliegen  <-ROOT-- fliegen\n",
      "morgen   <--mo--- fliegen\n",
      "nach     <--mo--- fliegen\n",
      "London   <--nk--- nach\n",
      ".        <-punct- fliegen\n",
      "Sie      <--sb--- sind\n",
      "sind     <-ROOT-- sind\n",
      "zum      <--mo--- sind\n",
      "ersten   <--nk--- Mal\n",
      "Mal      <--nk--- zum\n",
      "in       <--mo--- sind\n",
      "England  <--nk--- in\n",
      ".        <-punct- sind\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join('{child:<8} <{label:-^7} {head}'.format(child=t.orth_, label=t.dep_, head=t.head.orth_) for t in doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
