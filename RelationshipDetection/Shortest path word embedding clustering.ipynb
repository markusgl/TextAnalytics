{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest path relation extraction with word embeddings\n",
    "Approach:\n",
    "1. Search for Named Entities of type PERSON in sentence\n",
    "2. Build undirected graph of the dependecy path\n",
    "3. Search shortest path between each entity tuple\n",
    "4. Store words appearing on shortest path\n",
    "5. Get word embedding representation for each word on the shortest path (SP vectors)\n",
    "6. Sum SP vectors to a single vector\n",
    "7. Cluster vectors using hierarchical agglomerative clustering (HAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "import networkx as nx\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from networkx.exception import NodeNotFound, NetworkXNoPath\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Self-created data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'DE'\n",
    "utterance1 = u'''Herbert ist der Vater von Hans'''\n",
    "utterance2 = u'''Peter und Maria gehen morgen ins Kino'''\n",
    "utterance3 = u'''Herbert sein Sohn und ich gehen heute ins Kino'''\n",
    "utterance4 = u'''Ich gehe mit Johann in den Zoo'''\n",
    "utterance5 = u'''Hans und sein Sohn Hubert gehen in den Zoo.'''\n",
    "utterance6 = u'''Hans, welcher der Sohn von Hubert ist, geht mit Peter ins Kino.'''\n",
    "utterance7 = u'''Meine kleine Enkelin Lisa und mein Enkel Lukas fliegen morgen nach London.'''\n",
    "utterance8 = u'''Ich fahre mit meinen Enkeln Lukas und Lisa in den Urlaub.'''\n",
    "utterance9 = u'''Potesters seized several pumping stations, holding 127 Shell workers hostage.'''\n",
    "utterance10 = u'''Troops recently have raided churches, warning ministers to stop preaching.'''\n",
    "multi_utterances = u'''Homer und sein Sohn Peter gehen mit Milhouse ins Kino. Ich gehe mit Bart laufen.\n",
    "Meine Enkelin Lisa und mein Enkel Peter fliegen morgen nach London. Ned Flanders ist der Vater von Rod und Todd. \n",
    "Homer fährt mit seiner Tochter Lisa zum See. Homer fährt mit seiner Tochter Lisa zum See. Ich habe 3 Geschwister. \n",
    "Ich habe einen Bruder.'''\n",
    "\n",
    "text = multi_utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'EN'\n",
    "utterance1 = u'''my boyfriend and i are moving into an apartment together next week. i've a children and a dogs.\n",
    "                my sister Lindsey lives in New York. She works as a journalist. \n",
    "                i love going to the park with my three children and my wife.'''\n",
    "utterance2 = u'''My sister Lindsey lives in New York.'''\n",
    "utterance3 = u'''my father is an electrician. my father is a farmer.'''\n",
    "utterance4 = u'''my boyfriend and i are moving into an apartment together next week.'''\n",
    "utterance5 = u'''i've a children and a dogs.'''\n",
    "\n",
    "text = utterance1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Conversation Data from ConvAI coropus (EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First have a look at the data set and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sender_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-abefd2946da3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdialog\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dialog'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0msender\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdialog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sender_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdialog\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sender_class'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "with open('data/convai/export_2018-07-04_train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_columns = ['sender', 'text']\n",
    "df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "for row in data:\n",
    "    for dialog in row['dialog']:\n",
    "        sender = dialog['sender_class']\n",
    "        text = dialog['text']\n",
    "\n",
    "        data = {'sender': sender, 'text': text}\n",
    "        df = df.append(data, ignore_index=True)\n",
    "\n",
    "df[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract human dialogs and store to text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lang = 'EN'\n",
    "\n",
    "human_conv = df.loc[df['sender'] == 'Human']\n",
    "\n",
    "corpus = ''\n",
    "for row in human_conv['text']:\n",
    "    corpus += row.replace('\\n', '') + ' '\n",
    "  \n",
    "text = corpus[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simpsons conversations (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'EN'\n",
    "with open('data/simpsons_conversations.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.replace('\\n', ' ')\n",
    "    \n",
    "corpus = data[5000:10000]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Book of Genesis (EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'EN'\n",
    "with open('data/NETBible_Genesis.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "corpus = data[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Robinson Crusoe (DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Was für andere Gründe, sagte er, als die bloße Vorliebe für ein unstetes Leben, können dich bewegen, Vaterhaus und Heimat verlassen zu wollen, wo du dein gutes Unterkommen hast und bei Fleiß und Ausdauer in ruhigem und behaglichem Leben dein Glück machen kannst.\\nSie ist weder dem Elend und der Mühsal der nur von Händearbeit lebenden Menschenklasse ausgesetzt, noch wird sie von dem Hochmuth, der Ueppigkeit, dem Ehrgeiz und dem Neid, die in den höheren Sphären der Menschenwelt zu Hause sind, heimgesucht. Am besten, fügte er hinzu, kannst du die Glückseligkeit des Mittelstandes daraus erkennen, daß er von Allen, die ihm nicht angehören, beneidet wird.\\nAuch der Weise bezeugt, daß jener Stand der des wahren Glückes ist, indem er betet: Armuth und Reichthum gib mir nicht.\\nHabe nur darauf Acht, fuhr mein Vater fort, so wirst du finden, daß das Elend der Menschheit zumeist an die höheren und niederen Schichten der Gesellschaft vertheilt ist.\\nDieser Weg führt vielmehr in gelassener Behaglichkei'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = 'DE'\n",
    "with open('data/Robinson_Crusoe_conversations.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "corpus = data[:1000]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Winnetou Band 1 (DE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffDiese Zurückhaltung schien ihm aber keineswegs lieb zu sein; ich erinnere mich noch heut des zornigen Gesichtes, welches er mir eines Abends, als ich zu ihm kam, zeigte, und des Tones, in welchem er mich empfing, ohne auf mein ›good evening‹ zu antworten: Wo habt Ihr denn gestern gesteckt, Sir? Zu Hause. Und vorgestern? Auch zu Hause. Macht mir doch nichts weis! Es ist wahr, Mr. Henry. Pshaw!\\nSolche grüne Vögel, wie Ihr einer seid, bleiben nicht im Neste hocken; die stecken die Schnäbel überall hin, nur da nicht, wo sie hingehören! Und wo gehöre ich hin, wenn es Euch beliebt, es mir zu sagen? Hierher zu mir, verstanden!\\nHabe Euch schon lange einmal nach etwas fragen wollen. Warum habt Ihr es nicht getan? Weil ich nicht wollte.\\nHört Ihr es? Und wann wollt Ihr denn? Heute vielleicht. So fragt getrost nur zu, forderte ich ihn auf, indem ich mich hoch auf die Schraubenbank setzte, an welcher er arbeitete.\\nAls ob ich so ein Greenhorn, wie Ihr seid, erst um Erlaubnis fragen müßte, wenn ich mit ihm reden will! Greenhorn? antwortete ich, die Stirn in Falten ziehend, denn ich fühlte mich bedeutend verletzt.\\nIch will annehmen, Mr. Henry, daß dieses Wort Euch ohne Absicht und nur so herausgefahren ist! Bildet Euch doch nichts ein, Sir!\\nIhr könnt ja nicht einmal schießen! Er sagte dies in einem außerordentlich verächtlichen Tone und mit einer solchen Bestimmtheit, als ob er seiner Sache förmlich sicher sei.\\nHm! antwortete ich lächelnd.\\nIst dies vielleicht die Frage, welche Ihr mir vorlegen wolltet? Ja, die ist es.\\nNun antwortet doch einmal! Gebt mir ein gutes Gewehr in die Hand, so will ich antworten, eher nicht. Da legte er den Büchsenlauf, an welchem er schraubte, weg, stand auf, trat nahe an mich heran, fixierte mich mit verwunderten Augen und rief aus: Ein Gewehr in die Hand, Sir?\\nMeine Gewehre kommen nur in solche Hände, in denen ich mit ihnen Ehre einlegen kann! Solche hab ich, nickte ich ihm zu.\\nKönnte mich wirklich wild machen mit seiner Dreistigkeit! Ich ließ ihn gewähren, denn ich kannte ihn, zog eine Zigarre hervor und brannte sie an.\\nHabt Ihr denn jemals ein Gewehr in der Hand gehabt? Ich denke. Wann? Schon längst und oft. Auch angelegt und abgedrückt? Ja. Und getroffen? Natürlich! Da ließ er den Lauf, den er geprüft hatte, rasch sinken, sah mich wieder an und meinte: Ja, getroffen, natürlich, aber was? Das Ziel, ganz selbstverständlich. Was?\\nWollt Ihr mir das im Ernste aufbinden? Behaupten, aber nicht aufbinden; es ist wahr. Hol Euch der Teufel, Sir!\\nEs ist ein Bärentöter, der beste, den ich jemals in den Händen gehabt habe. Ich ging hin, langte die Büchse herab und legte sie an.\\nHalloo! rief er aus, indem er aufsprang.\\nBesitzt Ihr denn eine solche Körperkraft? Anstatt der Antwort nahm ich ihn unten bei der zugeknöpften Jacke und bei dem Hosenbund und hob ihn mit dem rechten Arm empor.\\nThunder-storm! schrie er auf.\\nIhr seid ja noch weit kräftiger als mein Bill. Euer Bill?\\nWer ist das? Er war mein Sohn, der lassen wir das!\\nIhr seid ihm ähnlich von Gestalt, habt beinahe dieselben Augen und auch denselben Zug um den Mund; darum bin ich Euch na, das geht Euch ja doch nichts an! Der Ausdruck tiefer Trauer hatte sich über sein Gesicht gebreitet; er fuhr mit der Hand über dasselbe und fuhr dann in munterem Tone fort: Aber, Sir, bei Eurer Muskelkraft ist es wirklich jammerschade, daß Ihr Euch so auf die Bücher geworfen habt.\\nHättet Euch körperlich üben sollen! Habe ich auch. Wirklich? Ja. Boxen? Wird drüben bei uns nicht getrieben.\\nAber im Turnen und Ringen mache ich mit. Reiten? Ja. Fechten? Habe ich Unterricht erteilt. Mann, schneidet nicht auf! Wollt Ihr es versuchen? Danke; habe genug von vorhin!\\nSetzt Euch wieder nieder! Er kehrte zu seiner Schraubenbank zurück, und ich tat dasselbe.\\nPlötzlich sah er von der Arbeit auf und fragte: Habt Ihr Mathematik getrieben? War eine meiner Lieblingswissenschaften. Arithmetik, Geometrie? Natürlich. Feldmesserei? Sogar außerordentlich gern.\\nBin sehr oft, ohne daß ich es notwendig hatte, mit dem Theodolit draußen herumgelaufen. Und könnt messen, wirklich messen? Ja.\\nIch habe mich sowohl an Horizontal-, als auch an Höhenmessungen oft beteiligt, obgleich ich nicht behaupten will, daß ich mich als ausgelernten Geodäten betrachte. Well sehr gut, sehr gut! Warum fragt Ihr danach, Mr. Henry? Weil ich eine Ursache dazu habe.\\nMuß vorher wissen hm, ja, muß vorher wissen, ob Ihr schießen könnt. So stellt mich auf die Probe! Werde es auch tun; ja, werde es tun; darauf könnt Ihr Euch verlassen.\\nWann beginnt Ihr morgen früh den Unterricht? Um acht Uhr. So kommt um sechs zu mir.\\nWollen hinauf auf den Schießstand gehen, wo ich meine Gewehre einschieße. Warum so früh? Weil ich nicht länger warten will.\\nJetzt genug davon, habe Anderes zu tun, was weit, weit wichtiger ist. Er schien mit dem Gewehrlaufe fertig zu sein und nahm aus einem Kasten ein polygones Eisenstück, dessen Ecken er abzufeilen begann.\\nIch war neugierig, zu erfahren, warum; darum fragte ich ihn: Soll das auch ein Gewehrteil '"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = 'DE'\n",
    "with open('data/Winnetou_Band1_conversations.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "corpus = data[:5000]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set language settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lang == 'DE':\n",
    "    nlp = spacy.load('de') # for various nlp tasks (pos, ner, dep-path etc.)\n",
    "    flair_ner_tagger_lang = 'de-ner'\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    stemmer_lang = 'german'\n",
    "    \n",
    "    relationship_list = ['vater', 'mutter', 'papa', 'papi', 'mama', 'mami', 'sohn', 'tochter', 'bruder', 'schwester', \n",
    "                         'enkel', 'enkelin', 'nichte', 'neffe', 'großvater', 'großmutter', 'opa', 'opa', \n",
    "                         'onkel', 'tante', 'cousin', 'cousine', 'schwager', 'schwägerin', 'Mann', 'Frau']\n",
    "    me_list = ['ich', mein', 'meine']\n",
    "\n",
    "elif lang == 'EN':\n",
    "    nlp = spacy.load('en')\n",
    "    flair_ner_tagger_lang = 'ner'\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer_lang = 'english'\n",
    "    \n",
    "    relationship_list = ['father', 'mother', 'dad', 'daddy', 'mom', 'mommy', 'son', 'daughter', 'brother', 'sister', \n",
    "                         'grandchild', 'grandson', 'granddaughter', 'grandfather', 'grandmother', \n",
    "                         'grampa', 'grandpa', 'grandma', 'niece', 'nephew', 'uncle', 'aunt', 'cousin'\n",
    "                        'brother-in-law', 'sister-in-law', 'husband', 'wife']\n",
    "    me_list = ['i', 'my']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "choose one of the two below (default=Flair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. spaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entities = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PER':\n",
    "        entities.append(ent.text.lower())\n",
    "\n",
    "for token in doc:\n",
    "    if token.text.lower() in me_list:\n",
    "        entities.append(token.text.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Flair NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\marku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# PER-PER entities\n",
    "def extract_per_to_per_entities(raw_sentence):\n",
    "    entities = []\n",
    "    \n",
    "    clean_sentence = re.sub('\\W+', ' ', raw_sentence) # remove non-word characters\n",
    "    sentence = Sentence(clean_sentence)\n",
    "    tagger = SequenceTagger.load(flair_ner_tagger_lang)\n",
    "    tagger.predict(sentence) # run NER over sentence\n",
    "    \n",
    "    # NER spans\n",
    "    print('Trying to extract entities...')\n",
    "    \n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        print(f'Entity: {entity}')\n",
    "\n",
    "        if entity.tag == 'PER':\n",
    "            if len(entity.tokens) > 1:  # if it is a multi word entity, replace blanks with underscores \n",
    "                entities.append(str(entity.text.lower()).replace(' ', '_'))\n",
    "            else:\n",
    "                entities.append(entity.text.lower())\n",
    "\n",
    "    # check for me or relationship\n",
    "    # Lemmatization\n",
    "    #if len(entities) == 0:\n",
    "    #    for token in sentence:\n",
    "    #        lemmatized_entity = lemmatizer.lemmatize(token.text.lower())\n",
    "\n",
    "    #        if token.text.lower() in me_list or lemmatized_entity in relationship_list:\n",
    "    #            entities.append(token.text.lower())\n",
    "\n",
    "    # NER tag for each token\n",
    "    #for token in sentence:\n",
    "    #    ner_tag = token.get_tag('ner')\n",
    "    #    print(f'{token}, {ner_tag}')\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build undirected graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_undirected_graph(sentence, plot=False):\n",
    "    doc = nlp(sentence)\n",
    "    edges = []\n",
    "    for token in doc:\n",
    "        for child in token.children:\n",
    "            edges.append((f'{token.lower_}',\n",
    "                          f'{child.lower_}'))\n",
    "    graph = nx.Graph(edges)\n",
    "    if plot:\n",
    "        plot_graph(graph)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(graph):\n",
    "    # nx.draw_networkx(graph, node_size=100, ode_color=range(len(graph)))\n",
    "    pos = nx.spring_layout(graph)  # positions for all nodes\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(graph, pos, node_size=200)\n",
    "    # edges\n",
    "    nx.draw_networkx_edges(graph, pos, width=1)\n",
    "    # labels\n",
    "    nx.draw_networkx_labels(graph, pos, font_size=12, font_family='sans-serif')\n",
    "\n",
    "    plt.axis('off')  # disable axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Dependency Path\n",
    "Find shortest dependency path between every found two entities in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence: \"Thats good for you, i'm not very into new tech I am go to gym and live on donations So vegan... i have dogs maybe i should told then that they may eat cheap salads insted of meat Dogs or vegan in office?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i', 'i', 'i']\n",
      "Processing sentence: \"Strange answer Hello /test Hey What do you do?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hello Hello Hello Hi Неllo Hello what is your name ?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"hello anybody there Thank you how are you /test /Test /test hey i'm from california.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"i'm a recording enineer and own my own studio.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'my']\n",
      "Processing sentence: \"where are you from?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hi Where is susan?\"\n",
      "##> Trying to extract entities...\n",
      "Entity: PER-span [4]: \"susan\"\n",
      "Entities ['susan']\n",
      "Processing sentence: \"Where do you live?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I live in colorado.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"I like rock climbing.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"What do you like to do.\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Lol Random answers.\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"My brother likes them too.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['my', 'brother']\n",
      "Processing sentence: \"Yeah, and you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"What kind of music?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Do you have dogs?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Cool, I love dogs.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"But i have siblings instead... Where do you live?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"I live in alabama I work from home what do you want me to say?\"\n",
      "##> Trying to extract entities...\n",
      "Entity: LOC-span [4]: \"alabama\"\n",
      "Entities ['i', 'i', 'me']\n",
      "Processing sentence: \"hey come on work yandex sucks i don't work from home i don't like to spend my money on cars i've never had a steady relationfish i watch too much tv from other peoples windows i don't go to the gym most days hello how are you are you ok?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i', 'my', 'i', 'i', 'i']\n",
      "Processing sentence: \"i have a dog too i enjoy american sports i am working in a company for 15 years Hi I like drinking tea while reading some books So you like it or hate it?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i', 'i', 'i']\n",
      "Processing sentence: \"Where do you live?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Close to mountains?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I like movies What is your job Hello there!\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Hi, is anyone on ohter end?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hello!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Is it interesting?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Do what?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"What are you doing now?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Me too.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['me']\n",
      "Processing sentence: \"I am moving my cup of tea with my mind.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'my', 'my']\n",
      "Processing sentence: \"Doctors say i am crazy and imagine things Not really... i spend a lot of time at the hospital.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i']\n",
      "Processing sentence: \"And you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Do you like music?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Cool.\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I should play piano too metallica i prefer metal music what is your job are you a musician how old are you?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i']\n",
      "Processing sentence: \"how old are you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"are you a musician?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Haha nice How old are you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Not the hamburger!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Not really... Are you a cook or what?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Can we talk about anything else?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Food is boooring Horses.\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I like horses.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"And you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"What tree?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Cook it hi how old are you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"are you ok?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"are you a sportsman?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"i dont like sports i like riding car do you have one?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i']\n",
      "Processing sentence: \"a horse?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"i have a rolls royce how old are you?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"what is your job?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"what do you do for living?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"hey what?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"why?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"are you ok who is er fu who's kong yuk ming no what is going on are you korean?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"what are you talking about no hey hello ?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"hey there talk to me, i need some rest oh, i have no kids i like playing ping pong do you like sports are you at work now Hello Pretty fine, and you?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['me', 'i', 'i', 'i']\n",
      "Processing sentence: \"Funny, because i too just finished doing same I think it's black, you know, like black in olives, and yours favourite?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i']\n",
      "Processing sentence: \"Oh i like thr band called Who, so what music do you generally listen?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Good, and where do you work?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Must be very intresting job, i work at local supermarket ani i hate it You contradicted yourself just now Ypu mean my message?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i', 'my']\n",
      "Processing sentence: \"Okay, i should go, bye Do you like parties?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"I'm a party animal myself.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Do you own any pets?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I own a cat and a dog.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Why don't you like dogs?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Have you ever had a cat?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Do you have any fear?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I am deathly afraid of heights.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Are you afraid of slim persons?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hi there!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Do you have short or long hair?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Ok Hi Hello Hello Hello?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Intresting, i have a rather big family and i love dogs That rather sad I am not a big fan of this 'healthy' food, i am rather fond of tacos though My passion is dogs I am living in Alabama, have a one big brother and three sisters Okey, this is nice but i should go, bye Hello!\"\n",
      "##> Trying to extract entities...\n",
      "Entity: LOC-span [40]: \"Alabama\"\n",
      "Entity: PER-span [49]: \"Okey\"\n",
      "Entities ['okey']\n",
      "Processing sentence: \"How are you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hello Hi Hello spiderman is my hero are you listening?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['my']\n",
      "Processing sentence: \"Hello there!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hi!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Anyone there?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hello Hey Are you there?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"It's fine hello Hello Are you there?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"/test Do you like to be indoors?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Do you love games?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"What do you do for living?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"How old are you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hey Answer Please Bye Hi, is there anybody, who speaks spanish?\"\n",
      "##> Trying to extract entities...\n",
      "Entity: MISC-span [11]: \"spanish\"\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I like leartning and studying languages.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"İ am a german teacher and sudying now spanish.\"\n",
      "##> Trying to extract entities...\n",
      "Entity: MISC-span [4]: \"german\"\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I wanna talk in spanish with u, to make my language skills better.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'my']\n",
      "Processing sentence: \"thats great, where do you learn spanish?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"which grade do u teaching?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"is it difficult ?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"and your students, do u like them?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"me too, i havent any kids too, but its agreat feeling to teaching to kids, i have a lot of good time and fun i teach in primary school yıour kids are jounger great, now i am understand and do u wanna learning other languages?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['me', 'i', 'i', 'i', 'i']\n",
      "Processing sentence: \"i like to playing gitar?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"writing some little songs, like a musician thats great, i liked it where do u come from?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"and do u like swimming and what about the beach, sand?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hello!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"How are you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"And I`m fine, thank you!\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"A have a lunch time and I`m free now for conversation.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Do you have free time to talk with me?\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['me']\n",
      "Processing sentence: \"I like to eat.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"But I have no money, my wife has spent it away.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'my', 'wife']\n",
      "Processing sentence: \"Yeah, it is a problem.\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"But now I work as a car salesman and I hope my balance will be better!\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'i', 'my']\n",
      "Processing sentence: \"Wonderful!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I have one for you!\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Pretty color for the best price!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"When could you take a look?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Maybe tomorrow morning at 9 a.m.?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Where do you work?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Are ypu married?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"Hi!\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"How are you?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I am going to build a new house.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Do you need it?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I could do it for you for free!\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"I enjoy buildong.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i']\n",
      "Processing sentence: \"Where are you from?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"I`d like to show you my cat.\"\n",
      "##> Trying to extract entities...\n",
      "Entities ['i', 'my']\n",
      "Processing sentence: \"Do you have one?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"hello do you like dogs?\"\n",
      "##> Trying to extract entities...\n",
      "No persons found in sentence\n",
      "Processing sentence: \"because i have a german shepherd dog my adopted husband\"\n",
      "##> Trying to extract entities...\n",
      "Entity: MISC-span [5]: \"german\"\n",
      "Entities ['i', 'my', 'husband']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'my-brother': [],\n",
       " 'i-wife': ['have', 'spent'],\n",
       " 'my-wife': [],\n",
       " 'i-husband': ['have'],\n",
       " 'my-husband': []}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dict = {}\n",
    "\n",
    "def search_shortest_dep_path(entities, sentence):\n",
    "    print(f'Searching shortest path between entities: {entities}')\n",
    "    graph = build_undirected_graph(sentence)\n",
    "\n",
    "    for i, first_entity in enumerate(entities):\n",
    "        first_entity = first_entity.split('_')[0]  # use only first name of multi-word entities\n",
    "\n",
    "        #for j in range(len(entities)):  # bidirectional relations\n",
    "        for j in range(i+1, len(entities)):  # unidirectional relations\n",
    "            second_entity = entities[j]\n",
    "            second_entity = second_entity.split('_')[0]  # use only first name of multi-word entities\n",
    "\n",
    "            if not i == j and second_entity not in me_list:        \n",
    "                try:\n",
    "                    shortest_path = nx.shortest_path(graph, source=first_entity, target=second_entity)\n",
    "                    key = first_entity + '-' + second_entity\n",
    "                    #path_dict[key] = shortest_path  # include entities in sp\n",
    "                    path_dict[key] = shortest_path[1:-1]  # exclude entities in sp\n",
    "                except NodeNotFound as err:\n",
    "                    logging.warning(f'Node not found: {err}')\n",
    "                except NetworkXNoPath as err:\n",
    "                    logging.warning(f'No path found: {err}')\n",
    "    else:\n",
    "        print('No persons found in sentence')\n",
    "\n",
    "    print('\\n')\n",
    "path_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical analysis (based on ReVerb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "            PP: {<PRON><AUX><DET><ADJ>?<NOUN>}\n",
    "            NP: {<DET><ADJ>?<NOUN><PROPN>*}            \n",
    "            REL: {<PP>|<NP>}\"\"\"\n",
    "\n",
    "grammar_en = r\"\"\"\n",
    "            PP: {<PRON><VERB><DET><ADJ>?<NOUN>}\n",
    "            NP: {<DET><ADJ>?<NOUN><PROPN>*}            \n",
    "            REL: {<PP>|<NP>}\"\"\"\n",
    "\n",
    "\n",
    "def search_rel_type(sentence):\n",
    "    rel_type = False\n",
    "\n",
    "    for token in word_tokenize(sentence):\n",
    "        if token.lower() in relationship_list:\n",
    "            rel_type = True\n",
    "\n",
    "    return rel_type\n",
    "\n",
    "\n",
    "def pos_tag_sentence(sentence):\n",
    "    sentence = re.sub('\\W+', ' ', sentence)\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    pos_tagged_sentence = []\n",
    "    for token in doc:\n",
    "        pos_tuple = (token.text, token.pos_)\n",
    "        pos_tagged_sentence.append(pos_tuple)\n",
    "\n",
    "    return pos_tagged_sentence\n",
    "\n",
    "\n",
    "def chunk_sentence(pos_tagged_sentence):\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    result = cp.parse(pos_tagged_sentence)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def extract_rel(sentence):\n",
    "    me = None\n",
    "    relative = None\n",
    "\n",
    "    if search_rel_type(sentence):\n",
    "        chunk = chunk_sentence(pos_tag_sentence(sentence))\n",
    "\n",
    "        if chunk[0][0][0][0].lower() in me_list:\n",
    "            me = chunk[0][0][0][0]\n",
    "            relative = chunk[0][0][-1][0]\n",
    "\n",
    "    return me, relative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction relations\n",
    "* if PER-PER in sentence, use shortest dependency path to extract the features in between\n",
    "* if not PER-PER in senctence, find description of a user related person through lexical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sent_tokenize(text):\n",
    "    print(f'##> Processing sentence: \"{sentence}\"')\n",
    "    entities = extract_per_to_per_entities(sentence)\n",
    "    \n",
    "    if entities:\n",
    "        print(f'Entities found: {entities}')\n",
    "        search_shortest_dep_path(entities, sentence)\n",
    "    else:\n",
    "        extract_rel(sentence)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['m1', 'm2', 'm1_pos', 'm2_pos', 'before_m1', 'after_m2', 'between_words', \n",
    "                   'short_path', 'm1_head', 'm2_head']\n",
    "\n",
    "features = pd.DataFrame(columns=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homer und sein sohn peter gehen mit milhouse ins kino \n",
      "ich gehe mit bart laufen \n",
      "meine enkelin lisa und mein enkel peter fliegen morgen nach london \n",
      "ned flanders ist der vater von rod und todd \n",
      "homer fährt mit seiner tochter lisa zum see \n"
     ]
    }
   ],
   "source": [
    "features_list = []  # for TF-IDF vectorization\n",
    "#ners = []\n",
    "\n",
    "# check for named entity 'PER' or 'PME' in each sentence\n",
    "#for ner_tuple in ner_tuples:\n",
    "#    if 'I-PER' in ner_tuple:\n",
    "#        ners.append(ner_tuple)\n",
    "#    elif ner_tuple[0].lower() in me_list:\n",
    "#        ners.append((ner_tuple[0], 'PME'))\n",
    "#    elif ner_tuple[0].lower() in relationship_list:\n",
    "#        ners.append((ner_tuple[0], 'SOC'))\n",
    "\n",
    "for sentence in sent_tokenize(text):\n",
    "    sentence = re.sub(r'\\W+', ' ', sentence.lower()) # remove non-word characters\n",
    "    print(sentence)\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    for key, value in path_dict.items():\n",
    "        # extract entities\n",
    "        m1 = key.split('-')[0]\n",
    "        m2 = key.split('-')[1]\n",
    "\n",
    "        short_path = value\n",
    "\n",
    "        # POS tagging and head\n",
    "        for token in doc:\n",
    "            if token.text.lower() == m1:\n",
    "                m1_pos_tag = token.pos_\n",
    "                m1_head = token.head.text\n",
    "                #m1_children = [child for child in token.children]\n",
    "            elif token.text.lower() == m2:\n",
    "                m2_pos_tag = token.pos_\n",
    "                m2_head = token.head.text\n",
    "                #m2_children = [child for child in token.children]\n",
    "\n",
    "        # Dependecy parsing\n",
    "        #dep_path = []\n",
    "        #for chunk in doc.noun_chunks:\n",
    "        #    if chunk.root.text.lower() == m1 or chunk.root.text.lower() == m2:\n",
    "        #        dep_path.append([chunk.root.text, chunk.root.dep_, chunk.root.head.text])  \n",
    "\n",
    "        # Between words\n",
    "        start_position_m1 = sentence.find(m1)\n",
    "        start_position_m2 = sentence.find(m2)\n",
    "\n",
    "        # verify if the words were found in the sentence\n",
    "        if not start_position_m1 == -1 and not start_position_m2 == -1:\n",
    "            start_position_between = start_position_m1 + len(m1) + 1\n",
    "            end_position_between = start_position_m2\n",
    "\n",
    "            between = sentence[start_position_between:end_position_between]\n",
    "            between_words = []\n",
    "            for word in word_tokenize(between):\n",
    "                between_words.append(word)\n",
    "\n",
    "            beforeM1 = sentence[:start_position_m1 - 1]\n",
    "            afterM2 = sentence[start_position_m2 + len(m2):]\n",
    "\n",
    "            beforeM1_list = word_tokenize(beforeM1)\n",
    "            afterM2_list = word_tokenize(afterM2)\n",
    "\n",
    "            data = {'m1': m1, 'm2': m2, 'm1_pos': m1_pos_tag, 'm2_pos': m2_pos_tag,\n",
    "                    'before_m1': beforeM1_list, 'after_m2': afterM2_list,\n",
    "                    'between_words': between_words, 'short_path': short_path,\n",
    "                     'm1_head': m1_head, 'm2_head': m2_head}\n",
    "\n",
    "            training_example = pd.Series(data, index=feature_columns)\n",
    "            features = features.append(training_example, ignore_index=True)\n",
    "            #context = [beforeM1, between, afterM2]\n",
    "            #features_list.append(context)\n",
    "\n",
    "#features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>m1_pos</th>\n",
       "      <th>m2_pos</th>\n",
       "      <th>before_m1</th>\n",
       "      <th>after_m2</th>\n",
       "      <th>between_words</th>\n",
       "      <th>short_path</th>\n",
       "      <th>m1_head</th>\n",
       "      <th>m2_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homer</td>\n",
       "      <td>peter</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>[homer, und, sein, sohn, peter, gehen, mit, mi...</td>\n",
       "      <td>[gehen, mit, milhouse, ins, kino]</td>\n",
       "      <td>[und, sein, sohn]</td>\n",
       "      <td>[und, sohn]</td>\n",
       "      <td>gehen</td>\n",
       "      <td>gehen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>homer</td>\n",
       "      <td>milhouse</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>[homer, und, sein, sohn, peter, gehen, mit, mi...</td>\n",
       "      <td>[ins, kino]</td>\n",
       "      <td>[und, sein, sohn, peter, gehen, mit]</td>\n",
       "      <td>[gehen, mit]</td>\n",
       "      <td>gehen</td>\n",
       "      <td>mit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peter</td>\n",
       "      <td>milhouse</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>[homer, und, sein, sohn]</td>\n",
       "      <td>[ins, kino]</td>\n",
       "      <td>[gehen, mit]</td>\n",
       "      <td>[sohn, und, homer, gehen, mit]</td>\n",
       "      <td>gehen</td>\n",
       "      <td>mit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lisa</td>\n",
       "      <td>peter</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>[meine, enkelin]</td>\n",
       "      <td>[fliegen, morgen, nach, london]</td>\n",
       "      <td>[und, mein, enkel]</td>\n",
       "      <td>[und, enkel]</td>\n",
       "      <td>meine</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ned</td>\n",
       "      <td>rod</td>\n",
       "      <td>X</td>\n",
       "      <td>X</td>\n",
       "      <td>[ned, flanders, ist, der, vater, von, rod, und...</td>\n",
       "      <td>[und, todd]</td>\n",
       "      <td>[flanders, ist, der, vater, von]</td>\n",
       "      <td>[flanders, ist, vater, von]</td>\n",
       "      <td>flanders</td>\n",
       "      <td>von</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ned</td>\n",
       "      <td>todd</td>\n",
       "      <td>X</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>[ned, flanders, ist, der, vater, von, rod, und...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[flanders, ist, der, vater, von, rod, und]</td>\n",
       "      <td>[flanders, ist, vater, von, rod, und]</td>\n",
       "      <td>flanders</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rod</td>\n",
       "      <td>todd</td>\n",
       "      <td>X</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>[ned, flanders, ist, der, vater, von]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[und]</td>\n",
       "      <td>[und]</td>\n",
       "      <td>von</td>\n",
       "      <td>und</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>homer</td>\n",
       "      <td>lisa</td>\n",
       "      <td>ADV</td>\n",
       "      <td>X</td>\n",
       "      <td>[homer, fährt, mit, seiner, tochter, lisa, zum...</td>\n",
       "      <td>[zum, see]</td>\n",
       "      <td>[fährt, mit, seiner, tochter]</td>\n",
       "      <td>[fährt, mit, tochter]</td>\n",
       "      <td>fährt</td>\n",
       "      <td>mit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      m1        m2 m1_pos m2_pos  \\\n",
       "0  homer     peter    ADJ    ADJ   \n",
       "1  homer  milhouse    ADJ    ADJ   \n",
       "2  peter  milhouse    ADJ    ADJ   \n",
       "3   lisa     peter   VERB    ADJ   \n",
       "4    ned       rod      X      X   \n",
       "5    ned      todd      X    ADJ   \n",
       "6    rod      todd      X    ADJ   \n",
       "7  homer      lisa    ADV      X   \n",
       "\n",
       "                                           before_m1  \\\n",
       "0  [homer, und, sein, sohn, peter, gehen, mit, mi...   \n",
       "1  [homer, und, sein, sohn, peter, gehen, mit, mi...   \n",
       "2                           [homer, und, sein, sohn]   \n",
       "3                                   [meine, enkelin]   \n",
       "4  [ned, flanders, ist, der, vater, von, rod, und...   \n",
       "5  [ned, flanders, ist, der, vater, von, rod, und...   \n",
       "6              [ned, flanders, ist, der, vater, von]   \n",
       "7  [homer, fährt, mit, seiner, tochter, lisa, zum...   \n",
       "\n",
       "                            after_m2  \\\n",
       "0  [gehen, mit, milhouse, ins, kino]   \n",
       "1                        [ins, kino]   \n",
       "2                        [ins, kino]   \n",
       "3    [fliegen, morgen, nach, london]   \n",
       "4                        [und, todd]   \n",
       "5                                 []   \n",
       "6                                 []   \n",
       "7                         [zum, see]   \n",
       "\n",
       "                                between_words  \\\n",
       "0                           [und, sein, sohn]   \n",
       "1        [und, sein, sohn, peter, gehen, mit]   \n",
       "2                                [gehen, mit]   \n",
       "3                          [und, mein, enkel]   \n",
       "4            [flanders, ist, der, vater, von]   \n",
       "5  [flanders, ist, der, vater, von, rod, und]   \n",
       "6                                       [und]   \n",
       "7               [fährt, mit, seiner, tochter]   \n",
       "\n",
       "                              short_path   m1_head m2_head  \n",
       "0                            [und, sohn]     gehen   gehen  \n",
       "1                           [gehen, mit]     gehen     mit  \n",
       "2         [sohn, und, homer, gehen, mit]     gehen     mit  \n",
       "3                           [und, enkel]     meine     und  \n",
       "4            [flanders, ist, vater, von]  flanders     von  \n",
       "5  [flanders, ist, vater, von, rod, und]  flanders     und  \n",
       "6                                  [und]       von     und  \n",
       "7                  [fährt, mit, tochter]     fährt     mit  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_feature_columns = ['m1', 'm2', 'short_path_vector', 'm1_head', 'm2_head']\n",
    "vector_features = pd.DataFrame(columns=vector_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings of the words inside the shortest path and sum them into a single vector\n",
    "* Two Representations: GermanWordEmbeddings and Flair Word Embeddings\n",
    "Choose one of the two below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. GermanWordEmbeddings https://github.com/devmount/GermanWordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\program files\\python\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"word 'homer' not in vocabulary\"\n",
      "\"word 'mit' not in vocabulary\"\n",
      "\"word 'milhouse' not in vocabulary\"\n",
      "\"word 'flanders' not in vocabulary\"\n",
      "\"word 'ist' not in vocabulary\"\n",
      "\"word 'von' not in vocabulary\"\n",
      "\"word 'flanders' not in vocabulary\"\n",
      "\"word 'ist' not in vocabulary\"\n",
      "\"word 'von' not in vocabulary\"\n",
      "\"word 'und' not in vocabulary\"\n",
      "\"word 'todd' not in vocabulary\"\n",
      "\"word 'und' not in vocabulary\"\n",
      "\"word 'todd' not in vocabulary\"\n",
      "\"word 'homer' not in vocabulary\"\n",
      "\"word 'fährt' not in vocabulary\"\n",
      "\"word 'mit' not in vocabulary\"\n",
      "\"word 'tochter' not in vocabulary\"\n",
      "\"word 'lisa' not in vocabulary\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1</th>\n",
       "      <th>m2</th>\n",
       "      <th>short_path_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homer</td>\n",
       "      <td>milhouse</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ned</td>\n",
       "      <td>rod</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ned</td>\n",
       "      <td>todd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rod</td>\n",
       "      <td>todd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>homer</td>\n",
       "      <td>lisa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      m1        m2 short_path_vector\n",
       "0  homer  milhouse               NaN\n",
       "1    ned       rod               NaN\n",
       "2    ned      todd               NaN\n",
       "3    rod      todd               NaN\n",
       "4  homer      lisa               NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_model = KeyedVectors.load_word2vec_format('../models/german.model', binary=True)\n",
    "\n",
    "embedding_vectors = []\n",
    "# get vector length\n",
    "vector_len = len(ger_model.wv['hallo'])\n",
    "\n",
    "for row in features.iterrows():\n",
    "    m1 = row[1]['m1']\n",
    "    m2 = row[1]['m2']\n",
    "    short_path = row[1]['short_path']\n",
    "    \n",
    "    # get the word embedding representation for each word in the shortest path\n",
    "    #row_embeddings = np.empty(len(short_path))\n",
    "    row_embeddings = []\n",
    "    \n",
    "    for word in short_path:\n",
    "        try:\n",
    "            row_embeddings.append(ger_model.wv[word])\n",
    "        except KeyError as err:\n",
    "            print(err)\n",
    "            row_embeddings.append(np.zeros(vector_len))\n",
    "    \n",
    "    #print(sum(row_embeddings))\n",
    "    embedding_vectors.append(sum(row_embeddings))\n",
    "    vector_data = {'m1': m1, 'm2': m2, 'short_path_embedding_vector': sum(row_embeddings)}\n",
    "    training_example = pd.Series(vector_data, index=vector_feature_columns)\n",
    "    vector_features = vector_features.append(training_example, ignore_index=True)\n",
    "\n",
    "# summarize vectors    \n",
    "#row_embeddings.sum()\n",
    "vector_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Flair Word Embeddings  https://github.com/zalandoresearch/flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence, Token\n",
    "from flair.embeddings import WordEmbeddings\n",
    "\n",
    "flair_embeddings = {}\n",
    "for sentence in sent_tokenize(text):\n",
    "    sentence = re.sub(r'\\W', ' ', sentence)\n",
    "    sentence = re.sub(r'\\s{2,}', ' ', sentence)    \n",
    "    \n",
    "    sentence = Sentence(sentence.lower())\n",
    "    glove_embedding = WordEmbeddings('de')\n",
    "    #glove_embedding = WordEmbeddings('de-crawl')\n",
    "    glove_embedding.embed(sentence)\n",
    "    \n",
    "    for token in sentence:\n",
    "        flair_embeddings[token.text] = token.embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop word found: und\n",
      "stop word found: mit\n",
      "stop word found: und\n",
      "stop word found: mit\n",
      "stop word found: und\n",
      "stop word found: ist\n",
      "stop word found: von\n",
      "stop word found: ist\n",
      "stop word found: von\n",
      "stop word found: und\n",
      "stop word found: und\n",
      "stop word found: mit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['homer-peter',\n",
       " 'homer-milhouse',\n",
       " 'peter-milhouse',\n",
       " 'lisa-peter',\n",
       " 'ned-rod',\n",
       " 'ned-todd',\n",
       " 'homer-lisa']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flair_embedding_vectors = []\n",
    "labels = []\n",
    "for row in features.iterrows():\n",
    "    m1 = row[1]['m1']\n",
    "    m2 = row[1]['m2']\n",
    "    short_path = row[1]['short_path']\n",
    "    m1_head = row[1]['m1_head']\n",
    "    m2_hed = row[1]['m2_head']\n",
    "    \n",
    "    # get the word embedding for each word in the shortest path\n",
    "    row_embeddings = []\n",
    "    \n",
    "    for word in short_path:\n",
    "        if word not in stop_words:  # exclude stop words\n",
    "            try:\n",
    "                #row_embeddings.append(flair_embeddings[word])\n",
    "                # weight embedding higher if it contains a relation\n",
    "                if word.lower() in relationship_list:\n",
    "                    row_embeddings.append(flair_embeddings[word]*1.8)\n",
    "                else:\n",
    "                    row_embeddings.append(flair_embeddings[word])\n",
    "            except KeyError as err:\n",
    "                print(err)\n",
    "                row_embeddings.append(np.zeros(vector_len))\n",
    "        else:\n",
    "            print(f'stop word found: {word}')\n",
    "    \n",
    "    # append word embeddings for head words\n",
    "    \"\"\"\n",
    "    if m1_head:\n",
    "        try:\n",
    "            row_embeddings.append(flair_embeddings[m1_head])\n",
    "        except KeyError as err:\n",
    "            print(err)\n",
    "    if m2_head:\n",
    "        try:\n",
    "            row_embeddings.append(flair_embeddings[m2_head])\n",
    "        except KeyError as err:\n",
    "            print(err)\n",
    "    \"\"\"\n",
    "    \n",
    "    if row_embeddings:\n",
    "        flair_embedding_vectors.append(sum(row_embeddings))\n",
    "        labels.append(m1+'-'+m2)\n",
    "        #vector_data = {'m1': m1, 'm2': m2, 'short_path_embedding_vector': sum(row_embeddings)}\n",
    "        #training_example = pd.Series(vector_data, index=vector_feature_columns)\n",
    "        #vector_features = vector_features.append(training_example, ignore_index=True)\n",
    "\n",
    "#vector_features\n",
    "#flair_embedding_vectors\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tuples of the word vector representations for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tuples = ()\n",
    "for vector in flair_embedding_vectors:\n",
    "    if not tuples:\n",
    "        tuples = (vector, )\n",
    "    else:\n",
    "        tuples = tuples + (vector, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot summarized word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-cb0f24fc9170>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 7 is out of bounds for axis 0 with size 7"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAD8CAYAAABZ/vJZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3Xt0VdXd7vHvJESuYrBguUi5eCBCkk0g4SaXgAJBYQBBqS+igh5FKIo6DpRYrAqjoi2+LV449bX2BQSOMAiCfasWLzWFIAoJRG4CiolVYiEgiSABcvmdP5LsgiSBrOywk/B8xmCYPfdcc/3WHpKHudbaazozQ0REpLLqBbsAERGpnRQgIiLiiQJEREQ8UYCIiIgnChAREfFEASIiIp4oQERExBMFiIiIeKIAERERT+oHY6ctWrSwDh06BGPXIiK1Vlpa2hEzaxnsOkoFJUA6dOhAampqMHYtIlJrOee+CnYNZ9MpLBER8aRWBkhmZiaRkZHBLqNSXn75ZV577TUAJk+eTFJSElA8Gzty5EgwSxMR8SQop7Bqs8LCQkJCQiq93dSpU6uhGhGR4KmVMxAo/kV+//33ExERwfDhw8nLyyM9PZ2+ffvi8/lISEjg2LFjAAwePJhHH32UQYMG0bVrV7Zu3cq4cePo3Lkzjz/+uH/M5cuX07t3b6Kjo3nggQcoLCwEoGnTpjzxxBP06dOHzZs3n1NHcnIycXFx/PznP6dLly4kJiayYsUKevfuTVRUFAcOHADgqaee4rnnnivzWF588UV69uxJVFQUe/fuBeC7775j7Nix+Hw++vbty44dO8ocJzIykszMTH744QdGjhxJ9+7diYyMZNWqVQCkpaURFxdHTEwM8fHxfPvtt4H4+EVEam+AfP7550yfPp3du3cTFhbGmjVruPvuu/ntb3/Ljh07iIqKYu7cuf7+V1xxBRs2bGDq1KmMGTOGRYsWsWvXLpYsWcLRo0f57LPPWLVqFZs2bSI9PZ2QkBBWrFgBwA8//EBkZCSffPIJAwYMOK+WTz/9lOeff56dO3eybNky9u/fz5YtW7jvvvt48cUXL3gsLVq0YNu2bUybNs0fDk8++SQ9evRgx44dzJ8/n7vvvrvCMf72t7/Rpk0bPv30U3bt2sWIESPIz8/noYceIikpibS0NO69917mzJlTmY9ZRKRctfYUVseOHYmOjgYgJiaGAwcOkJOTQ1xcHACTJk1i/Pjx/v6jR48GICoqioiICFq3bg1Ap06d+Prrr0lJSSEtLY1evXoBkJeXxzXXXANASEgIt956a7m19OrVyz/eddddx/Dhw/37+vDDDy94LOPGjfMfxxtvvAFASkoKa9asAeDGG2/k6NGj5ObmljtGVFQUM2fOZPbs2YwaNYqBAweya9cudu3axbBhw4DiWVtpnSIiVVVrAmTd9oMsWL+PrJw8rrZcTtu/r0OEhISQk5NT4fYNGjQAoF69ev6fS18XFBRgZkyaNIlnnnnmvG0bNmzov+7xySef8MADDwAwb948mjVrdt54Z++roKDggsdW2j8kJMTfv6yVIp1z1K9fn6KiIn/bqVOnAOjSpQtpaWm8/fbbPPbYYwwfPpyEhAQiIiLOO+0mIhIIteIU1rrtB3nsjZ0czMnDgEPfn+LQ96dYt/2gv89VV11F8+bN2bhxIwDLli3zz0Yuxk033URSUhKHDx8Giq9BfPXV+bdc9+nTh/T0dNLT0/2zmuowaNAg/ym05ORkWrRoQbNmzejQoQPbtm0DYNu2bWRkZACQlZVF48aNufPOO5k5cybbtm0jPDyc7Oxsf4Dk5+eze/fuaqtZRC4vtWIGsmD9PvLyC89pMzMWrN/H2B5t/W1Lly5l6tSpnDx5kk6dOrF48eKL3ke3bt34zW9+w/DhwykqKiI0NJRFixbRvn37gB1HZTz11FPcc889+Hw+GjduzNKlSwG49dZbee2114iOjqZXr1506dIFgJ07dzJr1izq1atHaGgof/zjH7niiitISkpixowZ5ObmUlBQwCOPPEJERERQjklE6hZX1qmS6hYbG2uV+SZ6x8S3KKtKB2Q8OzJgdYmI1GTOuTQziw12HaVqxSmsNmGNKtUuIiLVr1YEyKz4cBqFnvvlvUahIcyKDw9SRSIiUiuugZRe5yi9C6tNWCNmxYefc/1DREQurVoRIFAcIgoMEZGao1acwhIRkZpHASIiIp4oQERExBMFiIiIeKIAERERTxQgIiLiiQJEREQ8UYCIiIgnChAREfFEASIiIp4oQERExBMFiIiIeKIAERERTxQgIiLiScACxDkX4pzb7pz7a6DGFBGRmiuQM5CHgc8COJ6IiNRgAQkQ59y1wEjg1UCMJyIiNV+gZiALgV8CRQEaT0REargqB4hzbhRw2MzSLtBvinMu1TmXmp2dXdXdiohIkAViBtIfGO2cywRWAjc655b/uJOZvWJmsWYW27JlywDsVkREgqnKAWJmj5nZtWbWAfgP4O9mdmeVKxMRkRpN3wMRERFP6gdyMDNLBpIDOaaIiNRMmoGIiIgnChAREfFEASIiIp4oQERExBMFiIiIeKIAERERTxQgIiLiiQJEREQ8UYCIiIgnChAREfFEASIiIp4oQERExBMFiIiIeKIAERERTxQgIiLiiQJEREQ8UYCIiIgnChAREfFEASIiIp4oQERExBMFiIiIeKIAERERTxQgIiLiiQJEREQ8UYCIiIgnChAREfFEASIiIp4oQERExBMFiIiIeKIAERGpRZxzJ8ppH+uc61bJsTo453aV816ycy62ou0VICIidcNYoFIBUlUKEBGRSygzM5OuXbty//33ExERwfDhw8nLy+PAgQOMGDGCmJgYBg4cyN69ewHIyMigX79+9OrVC6BNWWM6524ARgMLnHPpzrnrnHPRzrmPnXM7nHNrnXPNS/rGOOc+dc5tBqafNUYj59zKkv6rgEYXOhYFiIjIJfb5558zffp0du/eTVhYGGvWrGHKlCm8+OKLpKWl8dxzz/GLX/wCgIcffphp06axdetWgPyyxjOzj4C/ALPMLNrMDgCvAbPNzAfsBJ4s6b4YmGFm/X40zDTgZEn/p4GYCx1H/UofuYiIVMq67QdZsH4fWTl5XG25XNOmHdHR0QDExMSQmZnJRx99xPjx4/3bnD59GoBNmzaxZs2a0uajwE8utD/n3FVAmJn9o6RpKbC6jPZlwM0lPw8CXgAwsx3OuR0X2o8CRESkGq3bfpDH3thJXn4hAIe+P8XRU8a67QcZ26MtISEhHDp0iLCwMNLT08scwzlXVtvTwEgAM4u+yHIcYBW8X9F759EpLBGRarRg/T5/eJQyMxas3+d/3axZMzp27Mjq1av973/66acA9O/fn5UrV5Z2/clZY8wpOV1VGh7HgStL3ssFjjnnBpa8dxfwDzPLAXKdcwNK2ieeVdaG0tfOuUjAd6Fjq3KAOOfaOec+dM595pzb7Zx7uKpjiojUFVk5eRfVvmLFCv785z/TvXt3IiIiePPNNwF4/vnnWbRoUelF9JAKdrUSmOWc2+6cuw6YRPFF9R1ANDCvpN89wKKSi+hnF/FHoGlJ/18CWy50bM6sUjOW8wdwrjXQ2sy2OeeuBNKAsWa2p7xtYmNjLTU1tUr7FRGpDfo/+3cOlhEibcMasSnxxkqN5ZxLM7MKv5txKVV5BmJm35rZtpKfjwOfAW2rOq6ISF0wKz6cRqHnThwahYYwKz48SBUFTkAvojvnOgA9gE/KeG8KMAXgZz/7WSB3KyJSY43tUfzv6dK7sNqENWJWfLi/vTar8iks/0DONQX+ATxtZm9U1FensETqlqZNm3LiRJlP2PCkQ4cOpKam0qJFi4CNWRfUuVNYAM65UGANsOJC4SEil6eCgoJglyABVuVTWK74BuU/A5+Z2e+rXpKIXGqZmZncfPPNDBgwgI8++oi2bdvy5ptvkpWVxfTp08nOzqZx48b86U9/4vrrrycjI4M77riDgoICRowYUe64kydP5uqrr2b79u307NmTOXPmcO+99/Lll1/SuHFjXnnlFXw+H0ePHmXChAlkZ2fTu3dvAnVmRKpXIGYg/Sm+x/jGkmewpDvnbgnAuCJyCXl9vEarVq0qHHf//v28//77/Od//idPPvkkPXr0YMeOHcyfP5+7774bgLlz5zJgwAC2b9/O6NGj+ec//1ntxytVV+UZiJmlUPztRhGpxTp27Ojp8Rp33XUXs2fPLnfc8ePHExJSfBdSSkqKf7sbb7yRo0ePkpuby4YNG3jjjeKz3yNHjqR58+aBP0AJOD3KROQy9ePnM522f99q6vXxGnPmzOGtt94C8G/XpEkT//tlnZoqHaes8aRm06NMRC5Dpc9nOpiTh1H8fKZD359i3faD/j4X+3iNFStW+Ld5+umnSU9PLzd0Bg0a5O+fnJxMixYtaNas2Tnt77zzDseOHQv4MUvgKUBELkMX83wmuLjHa+Tm5l70fp966ilSU1Px+XwkJiaydOlSAJ588kk2bNhAz549effdd/VdsVoiYN8DqQx9D0QkuDomvlXmY1cdkPHsyEtdjlykOvk9EBGpXdqElb3YXHntImVRgIhchury85nk0tFdWCKXobr8fCa5dBQgIpepsT3aKjCkSnQKS0REPFGAiIiIJwoQERHxRAEiIiKeKEBERMQTBYiIiHiiABEREU8UICIi4okCREREPFGAiIiIJwoQERHxRAEiIiKeKEBERMQTBYiIiHiiABEREU8UICIi4okCREREPFGAiIiIJwoQERHxRAEiIiKeKEBERMQTBYiIiHiiABEREU8UICIi4okCREREPFGAiIiIJwoQERHxRAEiIiKeBCRAnHMjnHP7nHNfOOcSAzGmiIjUbFUOEOdcCLAIuBnoBkxwznWr6rgiIlKzBWIG0hv4wsy+NLMzwEpgTADGFRGRGiwQAdIW+Pqs19+UtJ3DOTfFOZfqnEvNzs4OwG5FRCSYAhEgrow2O6/B7BUzizWz2JYtWwZgtyIiEkyBCJBvgHZnvb4WyArAuCIiUoMFIkC2Ap2dcx2dc1cA/wH8JQDjiohIDVa/qgOYWYFz7kFgPRAC/LeZ7a5yZSIiUqNVOUAAzOxt4O1AjCUiIrWDvokuIiKeKEBERMQTBchloGnTpgBkZWVx2223BaWG+fPnB2W/IlJ9nNl5X9modrGxsZaamnrJ93u5atq0KSdOnKh1NRQWFhISElJNFYnUPs65NDOLDXYdpTQDuYxkZmYSGRkJwO7du+nduzfR0dH4fD4+//xzAMaOHUtMTAwRERG88sorZY6zZMkSxowZw4gRIwgPD2fu3Ln+95YvX+4f94EHHqCwsJDExETy8vKIjo5m4sSJ5faD4qB54okn6NOnD5s3b67Oj0NEqsrMLvmfmJgYk0unSZMmZmaWkZFhERERZmb24IMP2vLly83M7PTp03by5EkzMzt69KiZmZ08edIiIiLsyJEj5423ePFia9WqlR05csTfb+vWrbZnzx4bNWqUnTlzxszMpk2bZkuXLj2nBjOrsB9gq1atCvhnIFIXAKkWhN/Z5f0JyG28UvOs236QBev3kZWTR15+Ieu2HyS6+b/f79evH08//TTffPMN48aNo3PnzgC88MILrF27FoCvv/6azz//nJ/85CfnjT9s2DB/+7hx40hJSaF+/fqkpaXRq1cvAPLy8rjmmmvO2/aDDz4ot19ISAi33npr4D4IEak2CpA6aN32gzz2xk7y8otPC5nBY2/s5NG+Yf4+d9xxB3369OGtt94iPj6eV199lXr16vH++++zefNmGjduzODBgzl16hRr1671n6Z69dVXAXDu3EegOecwMyZNmsQzzzxTYX0V9WvYsKGue4jUEroGUgctWL/PHx6l8vIL+a8NX/pff/nll3Tq1IkZM2YwevRoduzYQW5uLs2bN6dx48bs3buXjz/+GICEhATS09NJT08nNrb4+t17773Hd999R15eHuvWraN///7cdNNNJCUlcfjwYQC+++47vvrqKwBCQ0PJz88HqLCfiNQemoHUQVk5eWW2H/r+FKVzkFWrVrF8+XJCQ0Np1aoVTzzxBE2aNOHll1/G5/MRHh5O3759y93HgAEDuOuuu/jiiy+44447/MHym9/8huHDh1NUVERoaCiLFi2iffv2TJkyBZ/PR8+ePVmxYkW5/USk9tBtvHVQ/2f/zsEyQqRtWCM2Jd5Y5fGXLFlCamoqL730UpXHEpGLp9t4pdrNig+nUei51xEahYYwKz48SBWJSF2kGUgddfZdWG3CGjErPpyxPc5bKFJEapGaNgPRNZA6amyPtgoMEalWOoUlIiKeKEBERMQTBYiIiHiiABEREU8UICIi4okCREREPFGASJnOXjukNlq4cCEnT54MdhkidZoCRGqc0sWlqsJLgARivyKXEwWIlKuwsJD777+fiIgIhg8fTl5eHunp6fTt2xefz0dCQgLHjh0DYPDgwTz66KMMGjSIrl27snXrVv86I48//rh/TC8rESYnJzNo0CASEhLo1q0bU6dOpaioCIB3332Xfv360bNnT8aPH8+JEyd44YUXyMrKYsiQIQwZMqTcfgAdOnRg3rx5DBgwgNWrV1f7ZypSpwRjFSutSFjzZWRkWEhIiG3fvt3MzMaPH2/Lli2zqKgoS05ONjOzX//61/bwww+bmVlcXJz98pe/NDOzhQsXWuvWrS0rK8tOnTplbdu2tSNHjnheifDDDz+0Bg0a2IEDB6ygoMCGDh1qq1evtuzsbBs4cKCdOHHCzMyeffZZmzt3rpmZtW/f3rKzs83MLtjvt7/9bWA/PJFqglYklNqiY8eOREdHAxATE8OBAwfIyckhLi4OgEmTJjF+/Hh//9GjRwMQFRVFREQErVu3BqBTp058/fXXpKSkeF6JsHfv3nTq1AmACRMmkJKSQsOGDdmzZw/9+/cH4MyZM/Tr1++8bT/++OMK+91+++0ePh0RUYCI39kPYLzacjlt/36ib0hICDk5ORVu36BBAwDq1avn/7n0dUFBwUWvRPjJJ5/wwAMPADBv3jyaNWtW7gqIw4YN4/XXX6+wrgv1a9KkSYXbi0jZdA1EgH8vg3swJw+jePGpQ9+fYt32g/4+V111Fc2bN2fjxo0ALFu2zD8buRgXuxJhnz59/Csgls5qtmzZQkZGBkVFRaxatYoBAwbQt29fNm3axBdffAHAyZMn2b9/PwBXXnklx48fB6iwn4h4pwARoOxlcM2MBev3ndO2dOlSZs2ahc/nIz09nSeeeOKi99GtWzf/SoQ+n49hw4bx7bffXtS2/fr1IzExkcjISDp27EhCQgItW7ZkyZIlTJgwAZ/PR9++fdm7dy8AU6ZM4eabb2bIkCEV9hMR77QeiADQMfEtyvo/wQEZz4681OWcIzk5meeee46//vWvQa1DJNhq2nogmoEIAG3CGlWqXUREASJAzV4Gd/DgwZp9iNRAugtLAPyrF2oZXBG5WAoQ8dMyuCJSGTqFJSIinihARETEEwWIiIh4UqUAcc4tcM7tdc7tcM6tdc6FBaowERGp2ao6A3kPiDQzH7AfeKzqJYmISG1QpQAxs3fNrKDk5cfAtVUvSUREaoNAXgO5F3invDedc1Occ6nOudTs7OwA7lZERILhgt8Dcc69D7Qq4605ZvZmSZ85QAGworxxzOwV4BUofhaWp2pFRKTGuGCAmNnQit53zk0CRgE3WTCezCgiIkFRpW+iO+dGALOBODM7GZiSRESkNqjqNZCXgCuB95xz6c65lwNQk4iI1AJVmoGY2f8KVCEiIlK76JvoIiLiiQJEREQ8UYCIiIgnChAREfFEASIiIp4oQERExBMFiEgALFmyhKysrEuyr9TUVGbMmOHf74MPPgjA5MmTSUpKuiQ1iIDWRBcJiCVLlhAZGUmbNm0uepuCggLq16/8X8HY2FhiY2MrvZ1IoGkGIlKGzMxMrr/+eiZNmoTP5+O2227j5MmTpKWlERcXR0xMDPHx8Xz77bckJSWRmprKxIkTiY6OJi8vr8x+AIMHD+ZXv/oVcXFxPP/88+ftt2nTpsyePZuYmBiGDh3Kli1bGDx4MJ06deIvf/kLAMnJyYwaNarMujds2MANN9xAp06d/LMRM2PWrFlERkYSFRXFqlWryhznwQcfZMmSJQAkJibSrVs3fD4fM2fOBCA7O5tbb72VXr160atXLzZt2hSYD1tqLzO75H9iYmJMpCbLyMgwwFJSUszM7J577rHf/e531q9fPzt8+LCZma1cudLuueceMzOLi4uzrVu3mpnZmTNnKuw3bdq0cvcL2Ntvv21mZmPHjrVhw4bZmTNnLD093bp3725mZh9++KGNHDnSzMwWL15s06dPNzOzSZMm2W233WaFhYW2e/duu+6668zMLCkpyYYOHWoFBQX2r3/9y9q1a2dZWVnnjGNmNn36dFu8eLEdPXrUunTpYkVFRWZmduzYMTMzmzBhgm3cuNHMzL766iu7/vrrvX/A4gmQakH4nV3eH53CEilHu3bt6N+/PwB33nkn8+fPZ9euXQwbNgyAwsJCWrdufd52+/btq7Df7bffXu4+r7jiCkaMGAFAVFQUDRo0IDQ0lKioKDIzMy9Y89ixY6lXrx7dunXj0KFDAKSkpDBhwgRCQkL46U9/SlxcHFu3bqVZs2ZljtGsWTMaNmzIfffdx8iRI/2zlPfff589e/b4+33//fccP36cK6+88oJ1Sd2kABEpsW77QRas30dWTh5XWy6n8ovOef/KK68kIiKCzZs3VziOmVXYr0mTJkBxsMTExAAwevRo5s2bR2hoKM45AOrVq0eDBg38PxcUFJQ53tlK+5fWcfZ/f6x+/foUFf37GE+dOuVv37JlCx988AErV67kpZde4u9//ztFRUVs3ryZRo0aXbAOuTzoGogIxeHx2Bs7OZiThwGHvj9F9r8O8uyS4usOr7/+On379iU7O9sfDPn5+ezevRsoDpfjx48DEB4eXm6/s4WEhJCenk56ejrz5s2rtmMbNGgQq1atorCwkOzsbDZs2EDv3r1p3749e/bs4fTp0+Tm5vLBBx8AcOLECXJzc7nllltYuHAh6enpAAwfPpyXXnrJP25pu1y+NAMRARas30defuE5baE/acfCP/6J//f7x+ncuTMPPfQQ8fHxzJgxg9zcXAoKCnjkkUeIiIhg8uTJTJ06lUaNGrF582aSkpLK7BcMCQkJbN68me7du+Oc43e/+x2tWhUvMvrzn/8cn89H586d6dGjBwDHjx9nzJgxnDp1CjPjD3/4AwAvvPAC06dPx+fzUVBQwKBBg3j5Za3gcDlz5U1vq1NsbKylpqZe8v2KlKdj4luc/TehIPcQh5Pm0vZ//18ynh0ZtLpEzuacSzOzGnMPt05hiQBtwso+r19eu4goQEQAmBUfTqPQEP/r+lf9lOum/hez4sODWJVIzaZrICLA2B5tAfx3YbUJa8Ss+HB/u4icTwEiUmJsj7YKDJFK0CksERHxRAEiIiKeKEBERMQTBYiIiHiiABEREU8UICIi4okCREREPFGASFCcvZb3j82fP7/S4z311FM899xz57VnZmYSGRlZ6fFE5MIUIBJQZnbOGhNeeAkQEbn0FCBSZZmZmXTt2pVf/OIX9OzZk2XLlhEVFUVkZCSzZ8/291u8eDFdunQhLi6u3PW0ExMTycvLIzo6mokTJwLw+9//nsjISCIjI1m4cKG/79NPP014eDhDhw5l3759/va0tDS6d+9Ov379WLRoUTUdtYhoTXSpsoyMDHPO2ebNm+3gwYPWrl07O3z4sOXn59uQIUNs7dq1lpWV5W8/ffq03XDDDf61vH+sSZMm/p9TU1MtMjLSTpw4YcePH7du3brZtm3b/O0//PCD5ebm2nXXXWcLFiwwM7OoqChLTk42M7OZM2daRERE9X8IIpcAWhNd6qL27dvTt29f3nzzTQYPHkzLli0BmDhxIhs2bAA4p/32229n//79Fxw3JSWFhIQE/zKw48aNY+PGjRQVFZGQkEDjxo2B4iVhAXJzc8nJySEuLg6Au+66i3feeSewBysigB6mKB79eP3wwpDitbitggXKStf6PltZ64KfrbLjmVmZ7SISeLoGIpVW1vrhh74/xbrtB+nTpw//+Mc/OHLkCIWFhbz++uvExcXRp08fkpOTOXr0KPn5+axevRooe13w0NBQ8vPzgeL1vNetW8fJkyf54YcfWLt2LQMHDmTQoEGsXbuWvLw8jh8/zv/8z/8AEBYWxlVXXUVKSgoAK1asuPQfkMhlQjMQqbSy1g83Mxas38emxBt55plnGDJkCGbGLbfcwpgxY4DiW2379etH69at6dmzJ4WFhWUNz5QpU/D5fPTs2ZMVK1YwefJkevfuDcB9993nX7v79ttvJzo6mvbt2zNw4ED/9osXL+bee++lcePGxMfHV8dHICJoTXTx4Mfrh5dyoPXDRapRnVwT3Tk30zlnzrkWgRhPajatHy4iEIAAcc61A4YB/6x6OVIb/Hj9cIBGoSFaP1zkMhOIGcgfgF9CmWc1pA4a26Mtz4yLom1YIxzQNqwRz4yL0nKwIpeZKl1Ed86NBg6a2ae6dfLyovXDReSCAeKcex9oVcZbc4BfAcMvZkfOuSnAFICf/exnlShRRERqIs93YTnnooAPgJMlTdcCWUBvM/tXRdvqLiwRkcqraXdheT6FZWY7gWtKXzvnMoFYMzsSgLpERKSG0zfRRUTEk4B9E93MOgRqLBERqfmC8k1051w28NUl3/GFtQDq8im4unx8OrbaScdWOe3NrGWAx/QsKAFSUznnUmvSBapAq8vHp2OrnXRstZuugYiIiCcKEBER8UQBcq5Xgl1ANavLx6djq510bLWYroGIiIgnmoGIiIgnCpBy1MU1TpxzC5xze51zO5xza51zYcGuqaqccyOcc/ucc1845xKDXU+gOOfaOec+dM595pzb7Zx7ONg1BZpzLsQ5t90599dg1xJozrkw51xSyd+3z5xz/YJdU3VQgJShDq9x8h4QaWY+YD/wWJDrqRLnXAiwCLgZ6AZMcM51C25VAVMA/B8z6wr0BabXoWMr9TDHa6dJAAACM0lEQVTwWbCLqCbPA38zs+uB7tTR41SAlK1OrnFiZu+aWUHJy48pfgBmbdYb+MLMvjSzM8BKYEyQawoIM/vWzLaV/Hyc4l9Adeb5+c65a4GRwKvBriXQnHPNgEHAnwHM7IyZ5QS3quqhAPmRs9c4CXYt1exe4J1gF1FFbYGvz3r9DXXol2wp51wHoAfwSXArCaiFFP8jrSjYhVSDTkA2sLjkFN2rzrkmwS6qOgTsWVi1SaDWOKmJKjo2M3uzpM8cik+RrLiUtVWDslYxq1OzRudcU2AN8IiZfR/segLBOTcKOGxmac65wcGupxrUB3oCD5nZJ86554FE4NfBLSvwLssAMbOhZbWXrHHSEShdYfFaYJtz7oJrnNQU5R1bKefcJGAUcJPV/nu4vwHanfW6dE2aOsE5F0pxeKwwszeCXU8A9QdGO+duARoCzZxzy83sziDXFSjfAN+YWemMMYniAKlz9D2QCtS1NU6ccyOA3wNxZpYd7HqqyjlXn+KbAW4CDgJbgTvMbHdQCwsAV/wvmKXAd2b2SLDrqS4lM5CZZjYq2LUEknNuI3Cfme1zzj0FNDGzWUEuK+AuyxnIZewloAHwXskM62MzmxrckrwzswLn3IPAeiAE+O+6EB4l+gN3ATudc+klbb8ys7eDWJNcvIeAFc65K4AvgXuCXE+10AxEREQ80V1YIiLiiQJEREQ8UYCIiIgnChAREfFEASIiIp4oQERExBMFiIiIeKIAERERT/4/H+P4ObwteUQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.vstack(tuples)\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(path_dict.keys())\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dendrogram for choosing the right number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-130-423fdc35cb0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mdistance_sort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'descending'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             show_leaf_counts=True)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python\\lib\\site-packages\\scipy\\cluster\\hierarchy.py\u001b[0m in \u001b[0;36mdendrogram\u001b[1;34m(Z, p, truncate_mode, color_threshold, get_leaves, orientation, labels, count_sort, distance_sort, show_leaf_counts, no_plot, no_labels, leaf_font_size, leaf_rotation, leaf_label_func, show_contracted, link_color_func, ax, above_threshold_color)\u001b[0m\n\u001b[0;32m   2494\u001b[0m         \u001b[0mcontraction_marks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontraction_marks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2495\u001b[0m         \u001b[0mlink_color_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlink_color_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2496\u001b[1;33m         above_threshold_color=above_threshold_color)\n\u001b[0m\u001b[0;32m   2497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2498\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_plot\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python\\lib\\site-packages\\scipy\\cluster\\hierarchy.py\u001b[0m in \u001b[0;36m_dendrogram_calculate_info\u001b[1;34m(Z, p, truncate_mode, color_threshold, get_leaves, orientation, labels, count_sort, distance_sort, show_leaf_counts, i, iv, ivl, n, icoord_list, dcoord_list, lvs, mhr, current_color, color_list, currently_below_threshold, leaf_label_func, level, contraction_marks, link_color_func, above_threshold_color)\u001b[0m\n\u001b[0;32m   2747\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_marks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontraction_marks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2748\u001b[0m             \u001b[0mlink_color_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlink_color_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2749\u001b[1;33m             above_threshold_color=above_threshold_color)\n\u001b[0m\u001b[0;32m   2750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2751\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python\\lib\\site-packages\\scipy\\cluster\\hierarchy.py\u001b[0m in \u001b[0;36m_dendrogram_calculate_info\u001b[1;34m(Z, p, truncate_mode, color_threshold, get_leaves, orientation, labels, count_sort, distance_sort, show_leaf_counts, i, iv, ivl, n, icoord_list, dcoord_list, lvs, mhr, current_color, color_list, currently_below_threshold, leaf_label_func, level, contraction_marks, link_color_func, above_threshold_color)\u001b[0m\n\u001b[0;32m   2747\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_marks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontraction_marks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2748\u001b[0m             \u001b[0mlink_color_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlink_color_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2749\u001b[1;33m             above_threshold_color=above_threshold_color)\n\u001b[0m\u001b[0;32m   2750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2751\u001b[0m     \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python\\lib\\site-packages\\scipy\\cluster\\hierarchy.py\u001b[0m in \u001b[0;36m_dendrogram_calculate_info\u001b[1;34m(Z, p, truncate_mode, color_threshold, get_leaves, orientation, labels, count_sort, distance_sort, show_leaf_counts, i, iv, ivl, n, icoord_list, dcoord_list, lvs, mhr, current_color, color_list, currently_below_threshold, leaf_label_func, level, contraction_marks, link_color_func, above_threshold_color)\u001b[0m\n\u001b[0;32m   2659\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m         _append_singleton_leaf_node(Z, p, n, level, lvs, ivl,\n\u001b[1;32m-> 2661\u001b[1;33m                                     leaf_label_func, i, labels)\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python\\lib\\site-packages\\scipy\\cluster\\hierarchy.py\u001b[0m in \u001b[0;36m_append_singleton_leaf_node\u001b[1;34m(Z, p, n, level, lvs, ivl, leaf_label_func, i, labels)\u001b[0m\n\u001b[0;32m   2529\u001b[0m             \u001b[1;31m# for the leaf nodes, use it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2530\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2531\u001b[1;33m                 \u001b[0mivl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2532\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2533\u001b[0m                 \u001b[1;31m# Otherwise, use the id as the label for the leaf.x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x504 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage  \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "linked = linkage(X, 'single')\n",
    "\n",
    "#labelList = range(1, 11)\n",
    "\n",
    "plt.figure(figsize=(10, 7))  \n",
    "dendrogram(linked,  \n",
    "            orientation='top',\n",
    "            labels=words,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define number of clusters depending on the dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 5, 0, 2, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "#labels = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "labels = words\n",
    "agglo_clustering = AgglomerativeClustering(linkage='ward', affinity='euclidean', n_clusters=n)\n",
    "agglo_clustering.fit(X, labels)\n",
    "\n",
    "agglo_clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1f64cc331d0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0],X[:,1], c=agglo_clustering.labels_, cmap='rainbow')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 2, 1, 0, 1, 1, 3, 0, 1, 1, 0, 0, 0, 2, 2, 0, 1, 3, 3,\n",
       "       2, 1, 0, 2, 2, 1, 0, 0, 1, 3, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 2,\n",
       "       3, 2, 2, 0, 0, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 0, 2,\n",
       "       1, 1, 0, 0, 0, 2, 0, 3, 0, 1, 1, 1, 1, 2, 0, 0, 2, 2, 2, 0, 1, 3,\n",
       "       0, 2, 2, 2, 2, 0, 1, 2, 3, 1, 0, 0, 2, 1, 2, 1, 3, 1, 2, 0, 2, 2,\n",
       "       2, 3, 1, 0, 3, 2, 1, 2, 0, 1, 2, 2, 1, 3, 2, 3, 2, 2, 1, 2, 2, 1,\n",
       "       2, 1, 2, 1, 2, 3, 1, 2, 0, 2, 2, 3, 0, 0, 1, 1, 0, 1, 1, 2, 2, 1,\n",
       "       2, 2, 0, 3, 3, 1, 0, 1, 2, 1, 0, 1, 0, 2, 0, 2, 0, 0, 2, 2, 1, 1,\n",
       "       1, 0, 1, 0, 2, 1, 2, 0, 3, 1, 0, 2, 2, 2, 1, 1, 2, 2, 3, 2, 0, 1,\n",
       "       0, 2, 2, 2, 3, 2, 2, 3, 2, 0, 1, 2, 1, 2, 1, 1, 0, 3, 0, 0, 1, 1,\n",
       "       0, 2, 2, 2, 0, 1, 1, 2, 1, 3, 2, 2, 2, 2, 0, 0, 0, 0, 1, 2, 2, 2,\n",
       "       1, 2, 2, 2, 0, 2, 0, 3, 2, 1, 0, 2, 2, 0, 3, 2, 2, 2, 2, 2, 2, 3,\n",
       "       2, 2, 2, 1, 2, 0, 2, 1, 3, 0, 1, 0, 1, 0, 0, 3, 1, 3, 1, 1, 3, 0,\n",
       "       0, 0, 0, 2, 2, 2, 1, 3, 0, 0, 2, 2, 1, 3], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agglo_clustering.fit_predict(np.vstack(tuples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
